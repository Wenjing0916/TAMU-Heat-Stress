{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Digital Twin Framework with Spatiotemporal Vision Transformer for Heat Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Reference: Gong, W., Ye, X., Wu, K., Jamonnak, S., Zhang, W., Yang, Y., & Huang, X. (2025). [Integrating Spatiotemporal Vision Transformer into Digital Twins for High-Resolution Heat Stress Forecasting in Campus Environments](https://arxiv.org/abs/2502.09657). arXiv preprint arXiv:2502.09657.\n",
    "\n",
    "Note: GPU is required for training this model. This model was previously trained on two NVIDIA A800 Tensor Core GPUs (80GB memory each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Digital twin framework](Figure1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom PyTorch dataset class (CustomDataset) for spatiotemporal data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, spatial_data_dir, temporal_data_csv, output_data_dir, T_in, T_out):\n",
    "        self.spatial_data = []\n",
    "        self.output_data = []\n",
    "        self.spatial_data_max = None\n",
    "        self.spatial_data_min = None\n",
    "        spatial_files = sorted(os.listdir(spatial_data_dir))[:4]\n",
    "        for spatial_file in spatial_files:\n",
    "            spatial_image = np.array(Image.open(os.path.join(spatial_data_dir, spatial_file))).astype(np.float32)\n",
    "            spatial_image = torch.tensor(spatial_image).unsqueeze(0)  \n",
    "            spatial_image, self.spatial_data_max, self.spatial_data_min = self.maxminscaler_3d(spatial_image)  \n",
    "            spatial_image = TF.crop(spatial_image, top=0, left=0, height=3982, width=3739)\n",
    "            self.spatial_data.append(spatial_image)\n",
    "        self.spatial_data = torch.cat(self.spatial_data, dim=0)\n",
    "\n",
    "        temporal_data_df = pd.read_csv(temporal_data_csv).iloc[:, 1:]  \n",
    "        self.temporal_data = temporal_data_df.select_dtypes(include=[np.number]).fillna(0).astype(np.float32).values\n",
    "\n",
    "        self.temporal_data_max = np.max(self.temporal_data, axis=0)  \n",
    "        self.temporal_data_min = np.min(self.temporal_data, axis=0)  \n",
    "\n",
    "        num_samples_possible = (self.temporal_data.shape[0] // 336)\n",
    "        self.temporal_data = self.temporal_data[:num_samples_possible * 336].reshape(-1, 336, 7)  \n",
    "        self.temporal_data = self.normalize_temporal_data(self.temporal_data)\n",
    "\n",
    "        self.output_data_paths = [os.path.join(output_data_dir, f) for f in sorted(os.listdir(output_data_dir))]\n",
    "        for output_path in self.output_data_paths:\n",
    "            output_image = np.array(Image.open(output_path)).astype(np.float32)\n",
    "            self.output_data.append(output_image)\n",
    "        self.T_in = T_in\n",
    "        self.T_out = T_out\n",
    "        self.utci_max = None  \n",
    "        self.utci_min = None  \n",
    "\n",
    "        print(\"CustomDataset initialized successfully.\")\n",
    "        print(f\"Number of spatial images: {len(spatial_files)}\")\n",
    "        print(f\"Temporal data shape after reshape: {self.temporal_data.shape}\")\n",
    "        print(f\"Number of output images (UTCI): {len(self.output_data_paths)}\")\n",
    "        print(f\"T_in: {T_in}, T_out: {T_out}\")\n",
    "\n",
    "        self.compute_utci_global_max_min()\n",
    "        self.num_samples = self.temporal_data.shape[0] * (336 - (self.T_in + self.T_out - 1))\n",
    "        print(f\"Calculated num_samples: {self.num_samples}\")\n",
    "\n",
    "        if self.num_samples <= 0:\n",
    "            raise ValueError(\"Not enough time steps to generate input and output sequences\")\n",
    "\n",
    "    def normalize_temporal_data(self, temporal_data):\n",
    "        normalized_temporal_data = (temporal_data - self.temporal_data_min) / (self.temporal_data_max - self.temporal_data_min)\n",
    "        wind_direction_index = 4\n",
    "        normalized_temporal_data[:, :, wind_direction_index] = temporal_data[:, :, wind_direction_index] / 360.0\n",
    "\n",
    "        return normalized_temporal_data\n",
    "\n",
    "    def compute_utci_global_max_min(self):\n",
    "        for i, output_file in enumerate(self.output_data_paths):\n",
    "            output_data = self.output_data[i]\n",
    "            output_data_tensor = torch.tensor(output_data).unsqueeze(0)\n",
    "            current_max = output_data_tensor.max().item()\n",
    "            current_min = output_data_tensor.min().item()\n",
    "\n",
    "            if self.utci_max is None or current_max > self.utci_max:\n",
    "                self.utci_max = current_max\n",
    "            if self.utci_min is None or current_min < self.utci_min:\n",
    "                self.utci_min = current_min\n",
    "\n",
    "        print(f\"UTCI_Max: {self.utci_max}, UTCI_Min: {self.utci_min}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h_idx = random.randint(0,3917)\n",
    "        w_idx = random.randint(0,3674)\n",
    "        sample_idx = idx // (336 - (self.T_in + self.T_out - 1))\n",
    "        time_idx = idx % (336 - (self.T_in + self.T_out - 1))\n",
    "        spatial_data_seq = TF.crop(self.spatial_data, top=h_idx, left=w_idx, height=64, width=64).cpu()\n",
    "        spatial_data_seq = spatial_data_seq.unsqueeze(-1).repeat(1, 1, 1, self.T_in)  # [4, H, W, T_in]\n",
    "        spatial_data_seq = spatial_data_seq.permute(1, 2, 0, 3)  # [H, W, 4, T_in]\n",
    "\n",
    "        temporal_data = torch.tensor(self.temporal_data[sample_idx, time_idx:time_idx + self.T_in], dtype=torch.float32).cpu()\n",
    "\n",
    "        output_data_list = []\n",
    "        for t in range(self.T_out):\n",
    "            output_data = self.output_data[time_idx + self.T_in + t]\n",
    "            output_data = torch.tensor(output_data).unsqueeze(0)\n",
    "            output_data, _, _ = self.maxminscaler_3d(output_data, self.utci_max, self.utci_min)\n",
    "            output_data = TF.crop(output_data, top=h_idx, left=w_idx, height=64, width=64).cpu()\n",
    "            output_data_list.append(output_data)\n",
    "        output_data = torch.stack(output_data_list).permute(2, 3, 1, 0)\n",
    "        utci_input_list = []\n",
    "        for t in range(self.T_in):\n",
    "            utci_data = self.output_data[time_idx + t]\n",
    "            utci_data = torch.tensor(utci_data).unsqueeze(0)\n",
    "            utci_data, _, _ = self.maxminscaler_3d(utci_data, self.utci_max, self.utci_min)\n",
    "            utci_data = TF.crop(utci_data, top=h_idx, left=w_idx, height=64, width=64).cpu()\n",
    "            utci_input_list.append(utci_data)\n",
    "\n",
    "        utci_input = torch.stack(utci_input_list).permute(2, 3, 1, 0)  \n",
    "        spatial_data_seq = torch.cat([spatial_data_seq, utci_input], dim=2) \n",
    "\n",
    "        return spatial_data_seq.cpu(), output_data.cpu(), temporal_data.cpu()\n",
    "\n",
    "    def maxminscaler_3d(self, tensor_3d, scaler_max=None, scaler_min=None, range=(0, 1)):\n",
    "        if scaler_max is None:\n",
    "            scaler_max = tensor_3d.max()\n",
    "        if scaler_min is None:\n",
    "            scaler_min = tensor_3d.min()\n",
    "        X_std = (tensor_3d - scaler_min) / (scaler_max - scaler_min)\n",
    "        X_scaled = X_std * (range[1] - range[0]) + range[0]\n",
    "        return X_scaled, scaler_max, scaler_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a spatiotemporal Transformer-based decoder that leverages self-attention and cross-attention mechanisms to model spatial and temporal dependencies for structured sequence prediction tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism that handles inputs with additional dimensions.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, num_heads=4, mask=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mask = mask\n",
    "\n",
    "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
    "        self.head_dim = model_dim // num_heads\n",
    "\n",
    "        self.FC_Q = nn.Linear(model_dim, model_dim)\n",
    "        self.FC_K = nn.Linear(model_dim, model_dim)\n",
    "        self.FC_V = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # query, key, value: (batch_size, ..., length, model_dim)\n",
    "        batch_size = query.size(0)\n",
    "        extra_dims = query.size()[1:-2]  # Tuple of additional dimensions, if any\n",
    "        length = query.size(-2)\n",
    "\n",
    "        # Flatten extra dimensions into batch dimension for processing\n",
    "        if extra_dims:\n",
    "            new_batch_size = batch_size * int(torch.prod(torch.tensor(extra_dims)))\n",
    "            Q = self.FC_Q(query).view(new_batch_size, length, self.model_dim)\n",
    "            K = self.FC_K(key).view(new_batch_size, length, self.model_dim)\n",
    "            V = self.FC_V(value).view(new_batch_size, length, self.model_dim)\n",
    "        else:\n",
    "            Q = self.FC_Q(query).view(batch_size, length, self.model_dim)\n",
    "            K = self.FC_K(key).view(batch_size, length, self.model_dim)\n",
    "            V = self.FC_V(value).view(batch_size, length, self.model_dim)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(-1, length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size*, num_heads, length, head_dim)\n",
    "        K = K.view(-1, length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(-1, length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5  # (batch_size*, num_heads, length, length)\n",
    "\n",
    "        if self.mask:\n",
    "            seq_length = attn_scores.size(-1)\n",
    "            mask = torch.triu(torch.ones(seq_length, seq_length, device=query.device), diagonal=1).bool()\n",
    "            attn_scores.masked_fill_(mask, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)  # (batch_size*, num_heads, length, head_dim)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(-1, length, self.model_dim)\n",
    "\n",
    "        # Restore original batch and extra dimensions\n",
    "        if extra_dims:\n",
    "            attn_output = attn_output.view(batch_size, *extra_dims, length, self.model_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.view(batch_size, length, self.model_dim)\n",
    "\n",
    "        output = self.out_proj(attn_output)  # (batch_size, ..., length, model_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    \"\"\"Self-attention layer with residual connection and feed-forward network.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, feed_forward_dim=2048, num_heads=4, dropout=0.1, mask=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = AttentionLayer(model_dim, num_heads, mask)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(model_dim, feed_forward_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feed_forward_dim, model_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(model_dim)\n",
    "        self.ln2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, dim=-2):\n",
    "        # x: (batch_size, length, model_dim)\n",
    "        x = x.transpose(dim, -2)  # Bring the attention dimension to the second position\n",
    "        residual = x\n",
    "        out = self.attn(x, x, x)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.ln1(residual + out)\n",
    "\n",
    "        residual = out\n",
    "        out = self.feed_forward(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.ln2(residual + out)\n",
    "\n",
    "        out = out.transpose(dim, -2)  # Restore original dimensions\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    \"\"\"Cross-attention layer for decoder to attend to encoder outputs.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = model_dim // num_heads\n",
    "\n",
    "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Linear projections for query, key, and value\n",
    "        self.query_proj = nn.Linear(model_dim, model_dim)\n",
    "        self.key_proj = nn.Linear(model_dim, model_dim)\n",
    "        self.value_proj = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # query: (batch_size * H_W, T_out, model_dim)\n",
    "        # key, value: (batch_size * H_W, T_in, model_dim)\n",
    "        batch_size = query.size(0)\n",
    "        T_out = query.size(1)\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.query_proj(query)\n",
    "        K = self.key_proj(key)\n",
    "        V = self.value_proj(value)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, T_out, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, T_out, self.model_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Decoder layer with masked self-attention and cross-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, num_heads, feed_forward_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = SelfAttentionLayer(\n",
    "            model_dim, feed_forward_dim, num_heads, dropout, mask=True\n",
    "        )\n",
    "        self.cross_attn = CrossAttentionLayer(model_dim, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(model_dim, feed_forward_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feed_forward_dim, model_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(model_dim)\n",
    "        self.ln2 = nn.LayerNorm(model_dim)\n",
    "        self.ln3 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        batch_size, T_out, H_W, model_dim = x.size()\n",
    "        batch_size_mem, T_in, H_W_mem, model_dim_mem = memory.size()\n",
    "        assert H_W == H_W_mem, \"Spatial dimensions must match between x and memory\"\n",
    "        assert model_dim == model_dim_mem, \"Model dimensions must match\"\n",
    "\n",
    "        # Temporal self-attention with masking\n",
    "        residual = x\n",
    "        x = self.self_attn(x, dim=1)\n",
    "        x = self.ln1(x + residual)\n",
    "\n",
    "        # Cross-attention with encoder output\n",
    "        residual = x\n",
    "\n",
    "        # Reshape x and memory to (batch_size * H_W, T_out, model_dim)\n",
    "        x_reshaped = x.permute(0, 2, 1, 3).reshape(batch_size * H_W, T_out, model_dim)\n",
    "        memory_reshaped = memory.permute(0, 2, 1, 3).reshape(batch_size * H_W, T_in, model_dim)\n",
    "\n",
    "        x = self.cross_attn(x_reshaped, memory_reshaped, memory_reshaped)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        x = x.reshape(batch_size, H_W, T_out, model_dim).permute(0, 2, 1, 3)\n",
    "        x = self.ln2(x + residual)\n",
    "\n",
    "        # Feed-forward network\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.ln3(x + residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialTemporalTransformer_Decoder(nn.Module):\n",
    "    \"\"\"Decoder-only model with spatial and temporal attention.\"\"\"\n",
    "\n",
    "    def __init__(self, H, W, C_in, C_temp, T_in, C_out,\n",
    "                 hidden_dim=64, num_heads=4, num_layers=1, dropout=0.1):\n",
    "        super(SpatialTemporalTransformer_Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.C_out = C_out\n",
    "        self.T_in = T_in\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.H_W = H * W\n",
    "        self.C_temp = C_temp  # X_temp input features\n",
    "        self.C_in = C_in      # X_in input features\n",
    "\n",
    "        # Linear layer to project C_in to hidden_dim\n",
    "        self.fc_in = nn.Linear(self.C_in, self.hidden_dim)\n",
    "\n",
    "        # Linear layer to project C_temp to hidden_dim\n",
    "        self.fc_temp = nn.Linear(self.C_temp, self.hidden_dim)\n",
    "\n",
    "        # Spatial attention layers for X_in\n",
    "        self.spatial_attn_layers = nn.ModuleList([\n",
    "            SelfAttentionLayer(self.hidden_dim, feed_forward_dim=hidden_dim*4, num_heads=num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Temporal attention layers for X_in\n",
    "        self.temporal_attn_layers = nn.ModuleList([\n",
    "            SelfAttentionLayer(self.hidden_dim, feed_forward_dim=hidden_dim*4, num_heads=num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Temporal attention layers for X_temp\n",
    "        self.temp_temporal_attn_layers = nn.ModuleList([\n",
    "            SelfAttentionLayer(self.hidden_dim, feed_forward_dim=hidden_dim*4, num_heads=num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(self.hidden_dim, self.C_out)\n",
    "\n",
    "    def forward(self, X_in, X_temp):\n",
    "        batch_size = X_in.size(0)\n",
    "        H = self.H\n",
    "        W = self.W\n",
    "        H_W = H * W\n",
    "        T_in = self.T_in\n",
    "\n",
    "        # Process X_in\n",
    "        X_in = X_in.permute(0, 4, 1, 2, 3)  # (batch_size, T_in, H, W, C_in)\n",
    "        x_in = X_in.reshape(batch_size, T_in, H_W, self.C_in)  # (batch_size, T_in, H*W, C_in)\n",
    "        x_in = self.fc_in(x_in)  # (batch_size, T_in, H*W, hidden_dim)\n",
    "\n",
    "        # Apply spatial attention\n",
    "        for layer in self.spatial_attn_layers:\n",
    "            x_in = layer(x_in, dim=2)\n",
    "\n",
    "        # Apply temporal attention\n",
    "        for layer in self.temporal_attn_layers:\n",
    "            x_in = layer(x_in, dim=1)\n",
    "\n",
    "        # Process X_temp\n",
    "        x_temp = self.fc_temp(X_temp)  # (batch_size, T_in, hidden_dim)\n",
    "\n",
    "        # Apply temporal attention\n",
    "        for layer in self.temp_temporal_attn_layers:\n",
    "            x_temp = layer(x_temp, dim=1)\n",
    "\n",
    "        # Expand x_temp to match x_in's spatial dimensions\n",
    "        x_temp_expanded = x_temp.unsqueeze(2).repeat(1, 1, H_W, 1)  # (batch_size, T_in, H*W, hidden_dim)\n",
    "\n",
    "        # Combine x_in and x_temp\n",
    "        x_combined = x_in + x_temp_expanded  # (batch_size, T_in, H*W, hidden_dim)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.output_proj(x_combined)  # (batch_size, T_in, H*W, C_out)\n",
    "\n",
    "        # Reshape to (batch_size, H, W, C_out, T_in)\n",
    "        output = output.view(batch_size, T_in, H, W, self.C_out)\n",
    "        output = output.permute(0, 2, 3, 4, 1)  # (batch_size, H, W, C_out, T_in)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Spatiotemporal Vision Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm  \n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from dataloader import CustomDataset\n",
    "from Transformer import SpatialTemporalTransformer_Decoder\n",
    "from utils.utils import replace_w_sync_bn, CustomDataParallel\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_time(spatial_data_dir, temporal_data_csv, output_data_dir, train_ratio=0.7, val_ratio=0.15):\n",
    "    dataset = CustomDataset(spatial_data_dir, temporal_data_csv, output_data_dir, T_in=24, T_out=24)\n",
    "    total_samples = len(dataset) \n",
    "    train_size = int(train_ratio * total_samples) \n",
    "    val_size = int(val_ratio * total_samples)  \n",
    "    test_size = total_samples - train_size - val_size \n",
    "\n",
    "    train_indices = list(range(0, train_size))\n",
    "    val_indices = list(range(train_size, train_size + val_size))\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0005):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_and_evaluate_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.0001, log_dir='logs/1010_model', patience=10, min_delta=0.0005):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    model.apply(replace_w_sync_bn)\n",
    "    model = CustomDataParallel(model, 2)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, (spatial_data, output_data, temporal_data) in enumerate(tqdm(train_loader)):\n",
    "            spatial_data = spatial_data.to(device,non_blocking=True)\n",
    "            output_data = output_data.to(device,non_blocking=True)\n",
    "            temporal_data = temporal_data.to(device,non_blocking=True)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spatial_data, temporal_data)\n",
    "            loss = criterion(outputs, output_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), '../output/1010_model_trained_{}.pth'.format(epoch))\n",
    "            model.eval()  \n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():  \n",
    "                for spatial_data_val, output_data_val, temporal_data_val in val_loader:\n",
    "                    spatial_data_val = spatial_data_val.to(device,non_blocking=True)\n",
    "                    output_data_val = output_data_val.to(device,non_blocking=True)\n",
    "                    temporal_data_val = temporal_data_val.to(device,non_blocking=True)  \n",
    "                    outputs_val = model(spatial_data_val, temporal_data_val)\n",
    "                    loss_val = criterion(outputs_val, output_data_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    torch.save(model.module.state_dict(), '../output/1010_model_trained.pth')\n",
    "    print(\"Training complete, model saved.\")\n",
    "\n",
    "   \n",
    "    def calculate_metrics(loader, dataset_type):\n",
    "        model.eval()  \n",
    "        total_loss = 0.0\n",
    "        mse_criterion = nn.MSELoss()\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for spatial_data, output_data, temporal_data in loader:\n",
    "                spatial_data = spatial_data.to(device,non_blocking=True)\n",
    "                output_data = output_data.to(device,non_blocking=True)\n",
    "                temporal_data = temporal_data.to(device,non_blocking=True)  \n",
    "                outputs = model(spatial_data, temporal_data)\n",
    "                loss = mse_criterion(outputs, output_data)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        rmse = np.sqrt(avg_loss)\n",
    "        print(f\"{dataset_type} - MSE: {avg_loss:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    calculate_metrics(train_loader, \"Training Set\")\n",
    "    calculate_metrics(val_loader, \"Validation Set\")\n",
    "\n",
    "\n",
    "spatial_data_dir = '../data/spatial_images'\n",
    "temporal_data_csv = '../data/weather data.csv'\n",
    "output_data_dir = '../data/output_images'\n",
    "\n",
    "train_dataset, val_dataset = split_dataset_by_time(spatial_data_dir, temporal_data_csv, output_data_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, pin_memory=True, num_workers=10, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, pin_memory=True, num_workers=10, drop_last=True)\n",
    "\n",
    "H = 64\n",
    "W = 64\n",
    "C_in = 5\n",
    "T_in = 24\n",
    "C_temp = 7\n",
    "T_out = 24\n",
    "C_out = 1\n",
    "\n",
    "model = SpatialTemporalTransformer_Decoder(H=H, W=W, C_in=C_in, C_temp=C_temp, C_out=C_out,\n",
    "                                   T_in=T_in, hidden_dim=12, num_heads=2,\n",
    "                                   num_layers=1, dropout=0.1)\n",
    "\n",
    "train_and_evaluate_model(model, train_loader, val_loader, num_epochs=200, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import os\n",
    "import tifffile\n",
    "import csv\n",
    "from dataloader import CustomDataset\n",
    "from Transformer import SpatialTemporalTransformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scaler(tensor, scaler_max, scaler_min):\n",
    "    return tensor * (scaler_max - scaler_min) + scaler_min\n",
    "\n",
    "def save_predictions_to_csv(predictions, ground_truths, output_file, num_nodes, T_out):\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for node_id in range(num_nodes):  \n",
    "            row = [f\"{node_id+1}\"]  \n",
    "            for t in range(T_out):  \n",
    "                pred = predictions[node_id, t]\n",
    "                gt = ground_truths[node_id, t]\n",
    "                row.append(f\"{pred},{gt}\")  \n",
    "            writer.writerow(row)\n",
    "\n",
    "def predict_and_save(model, dataloader, output_file, num_nodes, T_out, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions_list = np.zeros((num_nodes, T_out))  \n",
    "    ground_truth_list = np.zeros((num_nodes, T_out))  \n",
    "    utci_max = dataloader.dataset.dataset.utci_max  \n",
    "    utci_min = dataloader.dataset.dataset.utci_min  \n",
    "    print(f\"Max UTCI: {utci_max}, Min UTCI: {utci_min}\")  \n",
    "    node_counter = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spatial_data, output_data, temporal_data in dataloader:\n",
    "            spatial_data = spatial_data.to(device)\n",
    "            output_data = output_data.to(device)\n",
    "            temporal_data = temporal_data.to(device)\n",
    "            print(np.shape(spatial_data), np.shape(temporal_data))\n",
    "            predictions = model(spatial_data, temporal_data)\n",
    "            predictions_np = predictions.cpu().detach().numpy()\n",
    "            output_data_np = output_data.cpu().detach().numpy()\n",
    "            print(f\"Predictions shape: {predictions_np.shape}\")\n",
    "            print(f\"Output data shape before transpose: {output_data_np.shape}\")\n",
    "            predictions_np = np.transpose(predictions_np, (0, 4, 1, 2, 3))\n",
    "\n",
    "            if len(output_data_np.shape) == 5 and output_data_np.shape[3] == 1:\n",
    "                output_data_np = np.transpose(output_data_np, (0, 4, 1, 2, 3))  \n",
    "\n",
    "            if predictions_np.shape[2] == 1:\n",
    "                predictions_np = np.squeeze(predictions_np, axis=2)  \n",
    "\n",
    "            if output_data_np.shape[4] == 1:\n",
    "                output_data_np = np.squeeze(output_data_np, axis=4)  \n",
    "\n",
    "            batch_size = predictions_np.shape[0] \n",
    "\n",
    "            for i in range(batch_size):  \n",
    "                for h in range(64): \n",
    "                    for w in range(64):  \n",
    "                        node_idx = node_counter + h * 64 + w  \n",
    "                        if node_idx < num_nodes:\n",
    "                            for t in range(T_out):  \n",
    "                                scalar_prediction = float(predictions_np[i, t, h, w])\n",
    "                                scalar_ground_truth = float(output_data_np[i, t, h, w])  \n",
    "\n",
    "                                predictions_list[node_idx, t] = inverse_scaler(scalar_prediction, utci_max, utci_min)\n",
    "                                ground_truth_list[node_idx, t] = inverse_scaler(scalar_ground_truth, utci_max, utci_min)\n",
    "\n",
    "            node_counter += 64 * 64  \n",
    "    print(np.shape(ground_truth_list))\n",
    "    save_predictions_to_csv(predictions_list, ground_truth_list, output_file, num_nodes, T_out)\n",
    "\n",
    "def split_dataset_by_time(spatial_data_dir, temporal_data_csv, output_data_dir, train_ratio=0.4, val_ratio=0.3):\n",
    "    dataset = CustomDataset(spatial_data_dir, temporal_data_csv, output_data_dir, T_in=24, T_out=24)\n",
    "    total_samples = len(dataset) \n",
    "    train_size = int(train_ratio * total_samples)  \n",
    "    val_size = int(val_ratio * total_samples)  \n",
    "    test_size = total_samples - train_size - val_size  \n",
    "    val_indices = list(range(train_size, train_size + val_size))\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_indices = list(range(train_size + val_size, total_samples))\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    return test_dataset\n",
    "\n",
    "spatial_data_dir = '../data/spatial_images'\n",
    "temporal_data_csv = '../data/weather data.csv'\n",
    "output_data_dir = '../data/output_images'\n",
    "\n",
    "test_dataset = split_dataset_by_time(spatial_data_dir, temporal_data_csv, output_data_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "H = 64\n",
    "W = 64\n",
    "C_in = 5\n",
    "T_in = 24\n",
    "C_temp = 7\n",
    "T_out = 24\n",
    "C_out = 1\n",
    "model = SpatialTemporalTransformer_Decoder(H=H, W=W, C_in=C_in, C_temp=C_temp, C_out=C_out,\n",
    "                                   T_in=T_in, hidden_dim=12, num_heads=2,\n",
    "                                   num_layers=1, dropout=0.1)\n",
    "\n",
    "checkpoint = torch.load('../output/1010_model_trained_40.pth')\n",
    "# model = checkpoint.module\n",
    "new_state_dict = {}\n",
    "for k,v in checkpoint.items():\n",
    "    new_state_dict[k[7:]] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.to('cuda')\n",
    "num_nodes = 64 * 64 * len(test_loader)  \n",
    "T_out = 24  \n",
    "output_file = '../output/1010_test_predictions.csv'\n",
    "predict_and_save(model, test_loader, output_file, num_nodes, T_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global MAPE: 0.051518, Global MAE: 1.638521, Global RMSE: 2.144549\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(file_name):\n",
    "    data = pd.read_csv(file_name, header=None)\n",
    "    global_sum_mape, global_sum_mae, global_sum_rmse = 0.0, 0.0, 0.0\n",
    "    global_num_mape, global_num_mae, global_num_rmse = 0, 0, 0\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        node_id = row[0] \n",
    "        for t in range(1, len(row)):\n",
    "            if row[t] != '':\n",
    "                pred, gt = map(float, row[t].split(','))\n",
    "                if gt != 0:  \n",
    "                    mape = abs(pred - gt) / abs(gt)  \n",
    "                    mae = abs(pred - gt)  \n",
    "                    rmse = (pred - gt) ** 2  \n",
    "                    global_sum_mape += mape\n",
    "                    global_sum_mae += mae\n",
    "                    global_sum_rmse += rmse\n",
    "                    global_num_mape += 1\n",
    "                    global_num_mae += 1\n",
    "                    global_num_rmse += 1\n",
    "\n",
    "    global_mape = global_sum_mape / global_num_mape if global_num_mape != 0 else np.nan\n",
    "    global_mae = global_sum_mae / global_num_mae if global_num_mae != 0 else np.nan\n",
    "    global_rmse = (global_sum_rmse / global_num_rmse) ** 0.5 if global_num_rmse != 0 else np.nan\n",
    "    print(f\"Global MAPE: {global_mape:.6f}, Global MAE: {global_mae:.6f}, Global RMSE: {global_rmse:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    calculate_accuracy('../output/1010_test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
